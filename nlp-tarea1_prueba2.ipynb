{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Tarea 1: Classificaton Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P0rfSVlNR2cq",
        "outputId": "56327ad3-6303-4056-bbfa-d4a38618faca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is CUDA available: True\n",
            "CUDA version: 12.6\n",
            "Number of GPUs available: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
          ]
        }
      ],
      "source": [
        "# Librer铆as\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "import torch\n",
        "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "\n",
        "from time import time\n",
        "from datasets import *\n",
        "from transformers import *\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhTfthoTfFW"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "El split **MNLI** del dataset **GLUE** consiste en un par de oraciones (premisa e hip贸tesis) y una etiqueta indicando la relaci贸n entre ellas:\n",
        "\n",
        "- _Entailment_: La hip贸tesis es una conclusi贸n l贸gica de la premisa.\n",
        "- _Neutral_: La hip贸tesis no puede ser determinada como verdadera o falsa basada en la premisa.\n",
        "- _Contradiction_: La hip贸tesis contradice la premisa.\n",
        "\n",
        "Adem谩s, este split contiene diferentes subconjuntos. Principalmente, usaremos el de _train_ para entrenar y los de _validation_ para evaluar la calidad del modelo. Los de _test_ los omitiremos para este trabajo.\n",
        "- _Train_: Dataset que usaremos para entrenar el modelo.\n",
        "- _MNLI-matched_ (MNLI-m): Dataset de validaci贸n creado a partir de las mismas categor铆as de los del conjunto de entrenamiento (e.g., noticias, ficci贸n).\n",
        "- _MNLI-mismatched_ (MNLI-mm): Dataset de validaci贸n creado a partir de diferentes categor铆as de los del conjunto de entrenamiento (e.g., discursos pol铆ticos, cartas).\n",
        "\n",
        "Aqu铆 la ficha del dataset para que pod谩is explorarla: https://huggingface.co/datasets/nyu-mll/glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "30aa4a06-0802-45be-ce2f-713fd9be98b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 392702\n",
              "    })\n",
              "    validation_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9815\n",
              "    })\n",
              "    validation_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9832\n",
              "    })\n",
              "    test_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9796\n",
              "    })\n",
              "    test_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9847\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKdCGQsR2cs"
      },
      "source": [
        "Con el 煤nico motivo de no demorar los tiempos de entrenamiento. Filtraremos el dataset y nos quedaremos solo con los registros que tenga longitud del campo _premise_ inferior a 20.\n",
        "\n",
        "El resto de la pr谩ctica se pide trabajarla sobre la variable `ds_tarea`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X6HrpprwIrIz",
        "outputId": "db7e2d20-b1b8-48a4-91fc-ae20440e8bf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 13635\n",
              "    })\n",
              "    validation_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 413\n",
              "    })\n",
              "    validation_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 296\n",
              "    })\n",
              "    test_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 382\n",
              "    })\n",
              "    test_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 288\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def filter_rows(x):\n",
        "    return len(x['premise'])<20\n",
        "ds_tarea = dataset.filter(filter_rows)\n",
        "\n",
        "assert len(ds_tarea['train']) == 13635\n",
        "assert len(ds_tarea['validation_matched']) == 413\n",
        "assert len(ds_tarea['validation_mismatched']) == 296\n",
        "\n",
        "ds_tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPepm7npTfFZ"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "En este apartado es donde tendr茅is que realizar todo el trabajo de la pr谩ctica. El formato, el an谩lisis, el modelo escogido y cualquier proceso intermedio que consider茅is es totalmente libre. Sin embargo, hay algunas pautas que tendr茅is que cumplir:\n",
        "\n",
        "- La variable `model_checkpoint` debe almacenar el nombre del modelo y el tokenizador de  que vais a utilizar.\n",
        "- La variable `model` y la variable `tokenizer` almacenar谩n, respectivamente, el modelo y el tokenizador de  que vais a utilizar.\n",
        "- La variable `trainer` almacenar谩 el _Trainer_ de  que, en la siguiente secci贸n utilizar茅is para entrenar el modelo.\n",
        "- Debe existir una funci贸n llamada `preprocess_function` que realice la tokenizaci贸n y, si lo consider谩is oportuno, transformaciones de las _features_.\n",
        "\n",
        "Nota: En el _tokenizer_, es obligatorio que el argumento `padding` sea distinto de `False` y que su salida sean **tensores** de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "38010333398d4bf3871e075b7ec4c39a"
          ]
        },
        "id": "ACSgiNELR2cs",
        "outputId": "302c5dff-f8f1-4ed6-a93a-ea722d36f612"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
            "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
            "* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n",
            "* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\n",
            "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38010333398d4bf3871e075b7ec4c39a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columnas del dataset tokenizado: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokeniza pares de oraciones (premise, hypothesis) para NLI.\n",
        "\n",
        "    Args:\n",
        "        examples: Batch de ejemplos con campos 'premise' y 'hypothesis'\n",
        "\n",
        "    Returns:\n",
        "        Dict con tensores tokenizados listos para el modelo\n",
        "    \"\"\"\n",
        "    # Tokenizaci贸n con truncation, padding y return_tensors=\"pt\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Agregar las etiquetas como tensor de PyTorch\n",
        "    if isinstance(examples[\"label\"], list):\n",
        "        tokenized[\"labels\"] = torch.tensor(examples[\"label\"])\n",
        "    else:\n",
        "        tokenized[\"labels\"] = torch.tensor([examples[\"label\"]])\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Aplicar preprocesamiento al dataset\n",
        "tokenized_datasets = ds_tarea.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=['premise', 'hypothesis', 'idx']\n",
        ")\n",
        "\n",
        "# Verificar que el dataset tiene las columnas correctas\n",
        "print(\"Columnas del dataset tokenizado:\", tokenized_datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PZJiowOVR2cs",
        "outputId": "f04eaf77-629e-481f-a381-13f606d6864a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n",
            "/tmp/ipython-input-2921160731.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Configuraci贸n del Entrenamiento\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Calcula m茅tricas de evaluaci贸n para el modelo.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Argumentos de entrenamiento optimizados\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_tarea1\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    warmup_steps=200,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    seed=42,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=None,\n",
        "    gradient_accumulation_steps=2,\n",
        "    dataloader_drop_last=True,\n",
        "    eval_accumulation_steps=1\n",
        ")\n",
        "\n",
        "# Inicializar el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ez5mU3s6R2ct",
        "outputId": "8df0d76b-61a9-4063-d07d-5982ceac03ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-988657807.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "# Data Collator para padding din谩mico (m谩s eficiente)\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Actualizar el trainer con el data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RRjTryA7R2ct"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uNx5pyRlIrJh",
        "scrolled": true,
        "outputId": "5f8c4de8-123d-4181-9f51-ab5763fe7dfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 13,635\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 3,408\n",
            "  Number of trainable parameters = 109,484,547\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdph0003\u001b[0m (\u001b[33mdph0003-complutense-university-of-madrid\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250922_225803-l2h1krmc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface/runs/l2h1krmc' target=\"_blank\">charmed-thunder-7</a></strong> to <a href='https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface' target=\"_blank\">https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface/runs/l2h1krmc' target=\"_blank\">https://wandb.ai/dph0003-complutense-university-of-madrid/huggingface/runs/l2h1krmc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3408' max='3408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3408/3408 14:48, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.123900</td>\n",
              "      <td>1.086109</td>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.363899</td>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.338708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.997700</td>\n",
              "      <td>0.926251</td>\n",
              "      <td>0.591146</td>\n",
              "      <td>0.611139</td>\n",
              "      <td>0.591146</td>\n",
              "      <td>0.587746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.784400</td>\n",
              "      <td>0.678762</td>\n",
              "      <td>0.726562</td>\n",
              "      <td>0.730612</td>\n",
              "      <td>0.726562</td>\n",
              "      <td>0.726321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.784800</td>\n",
              "      <td>0.599900</td>\n",
              "      <td>0.776042</td>\n",
              "      <td>0.780603</td>\n",
              "      <td>0.776042</td>\n",
              "      <td>0.776270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.636700</td>\n",
              "      <td>0.580545</td>\n",
              "      <td>0.763021</td>\n",
              "      <td>0.769534</td>\n",
              "      <td>0.763021</td>\n",
              "      <td>0.762022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.707900</td>\n",
              "      <td>0.565131</td>\n",
              "      <td>0.776042</td>\n",
              "      <td>0.780577</td>\n",
              "      <td>0.776042</td>\n",
              "      <td>0.777157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.599600</td>\n",
              "      <td>0.517377</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>0.804440</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>0.798357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.550900</td>\n",
              "      <td>0.526166</td>\n",
              "      <td>0.802083</td>\n",
              "      <td>0.810953</td>\n",
              "      <td>0.802083</td>\n",
              "      <td>0.803468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.473600</td>\n",
              "      <td>0.522097</td>\n",
              "      <td>0.822917</td>\n",
              "      <td>0.825130</td>\n",
              "      <td>0.822917</td>\n",
              "      <td>0.823371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.440200</td>\n",
              "      <td>0.536005</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.824630</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.821426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.458700</td>\n",
              "      <td>0.510976</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.834210</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.828755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.423300</td>\n",
              "      <td>0.551148</td>\n",
              "      <td>0.802083</td>\n",
              "      <td>0.809799</td>\n",
              "      <td>0.802083</td>\n",
              "      <td>0.803863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.448300</td>\n",
              "      <td>0.533715</td>\n",
              "      <td>0.809896</td>\n",
              "      <td>0.814651</td>\n",
              "      <td>0.809896</td>\n",
              "      <td>0.809496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.403300</td>\n",
              "      <td>0.503283</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.844244</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.837297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.432700</td>\n",
              "      <td>0.494280</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.822430</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.820803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.446700</td>\n",
              "      <td>0.487957</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.832659</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.831369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.456600</td>\n",
              "      <td>0.500802</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.835495</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.831997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.244600</td>\n",
              "      <td>0.592809</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.828770</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.826528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.267700</td>\n",
              "      <td>0.591944</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.821985</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.820433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.638079</td>\n",
              "      <td>0.817708</td>\n",
              "      <td>0.820420</td>\n",
              "      <td>0.817708</td>\n",
              "      <td>0.818279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.217600</td>\n",
              "      <td>0.657991</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.834306</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.827711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.255300</td>\n",
              "      <td>0.610136</td>\n",
              "      <td>0.815104</td>\n",
              "      <td>0.816492</td>\n",
              "      <td>0.815104</td>\n",
              "      <td>0.815237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.295900</td>\n",
              "      <td>0.606135</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.825378</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.821306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.542397</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833697</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.239300</td>\n",
              "      <td>0.622289</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.834155</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.827363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.131500</td>\n",
              "      <td>0.648084</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.830712</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.828730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.162000</td>\n",
              "      <td>0.665065</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.835881</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.695669</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.833247</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.831436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.140700</td>\n",
              "      <td>0.693652</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.837211</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.836238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.137900</td>\n",
              "      <td>0.705206</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833329</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.127700</td>\n",
              "      <td>0.715041</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.837066</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.836135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.706233</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.826963</td>\n",
              "      <td>0.825521</td>\n",
              "      <td>0.826030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.128900</td>\n",
              "      <td>0.715949</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.829244</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.828444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.122000</td>\n",
              "      <td>0.719326</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.824169</td>\n",
              "      <td>0.820312</td>\n",
              "      <td>0.821484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-200\n",
            "Configuration saved in ./results_tarea1/checkpoint-200/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-200/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-200/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-400\n",
            "Configuration saved in ./results_tarea1/checkpoint-400/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-400/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-400/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-600\n",
            "Configuration saved in ./results_tarea1/checkpoint-600/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-600/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-600/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-600/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-800\n",
            "Configuration saved in ./results_tarea1/checkpoint-800/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-800/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-800/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-800/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-1000\n",
            "Configuration saved in ./results_tarea1/checkpoint-1000/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-1000/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-1000/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-1200\n",
            "Configuration saved in ./results_tarea1/checkpoint-1200/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-1200/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-1200/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-1200/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-1400\n",
            "Configuration saved in ./results_tarea1/checkpoint-1400/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-1400/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-1400/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-1400/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-1600\n",
            "Configuration saved in ./results_tarea1/checkpoint-1600/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-1600/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-1600/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-1600/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-1800\n",
            "Configuration saved in ./results_tarea1/checkpoint-1800/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-1800/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-1800/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-1800/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-2000\n",
            "Configuration saved in ./results_tarea1/checkpoint-2000/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-2000/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-2000/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-2200\n",
            "Configuration saved in ./results_tarea1/checkpoint-2200/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-2200/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-2200/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-2200/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-2400\n",
            "Configuration saved in ./results_tarea1/checkpoint-2400/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-2400/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-2400/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-2400/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-2600\n",
            "Configuration saved in ./results_tarea1/checkpoint-2600/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-2600/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-2600/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-2600/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-2800\n",
            "Configuration saved in ./results_tarea1/checkpoint-2800/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-2800/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-2800/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-2800/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-3000\n",
            "Configuration saved in ./results_tarea1/checkpoint-3000/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-3000/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-3000/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-3200\n",
            "Configuration saved in ./results_tarea1/checkpoint-3200/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-3200/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-3200/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-3200/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 413\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-3400\n",
            "Configuration saved in ./results_tarea1/checkpoint-3400/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-3400/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-3400/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-3400/special_tokens_map.json\n",
            "Saving model checkpoint to ./results_tarea1/checkpoint-3408\n",
            "Configuration saved in ./results_tarea1/checkpoint-3408/config.json\n",
            "Model weights saved in ./results_tarea1/checkpoint-3408/model.safetensors\n",
            "tokenizer config file saved in ./results_tarea1/checkpoint-3408/tokenizer_config.json\n",
            "Special tokens file saved in ./results_tarea1/checkpoint-3408/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_tarea1/checkpoint-1400 (score: 0.8359375).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>>>>>>>>>>>> elapsed time: 15m\n"
          ]
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "start = time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end = time()\n",
        "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-mHF_60R2ct"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-hIByWWER2ct",
        "outputId": "fb0b3563-0af7-45b6-e738-32404cb7e9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** EVALUACIN ****\n",
            "********\n",
            "Tokenizer config:\n",
            "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "\n",
            "\n",
            "********\n",
            "Model config:\n",
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "********\n",
            "Trainer arguments:\n",
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=1,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=100,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results_tarea1,\n",
            "overwrite_output_dir=False,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=200,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=200,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "print(f\"**** EVALUACIN ****\")\n",
        "print(f\"********\\nTokenizer config:\\n{tokenizer}\")\n",
        "print(f\"\\n\\n********\\nModel config:\\n{model.config}\")\n",
        "print(f\"\\n\\n********\\nTrainer arguments:\\n{trainer.args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J4cv_VLsR2ct",
        "outputId": "2e4eaa6d-208c-4231-a703-878d7418f233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids es una instancia de torch.Tensor\n",
            "token_type_ids es una instancia de torch.Tensor\n",
            "attention_mask es una instancia de torch.Tensor\n",
            "labels es una instancia de torch.Tensor\n"
          ]
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "sample = ds_tarea['validation_matched'][0]\n",
        "inputs = preprocess_function(sample)\n",
        "for key, value in inputs.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key} es una instancia de torch.Tensor\")\n",
        "    else:\n",
        "        print(f\"{key} no es una instancia de torch.Tensor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "cf935e3d8ea14e84994de02baab5e713",
            "4371b3632bef43babf553566b8a50741",
            "5eaae3eda942461c91cce11d16266b03",
            "1fc3019f30884579bd7fe2ed3a317bcb"
          ]
        },
        "id": "PieSX4MWR2ct",
        "outputId": "b2ba4961-0116-46c5-9c39-a1751a2d65ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter 'function'=<function predict at 0x7f29c055c7c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function predict at 0x7f29c055c7c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf935e3d8ea14e84994de02baab5e713",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13635 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4371b3632bef43babf553566b8a50741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/413 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eaae3eda942461c91cce11d16266b03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fc3019f30884579bd7fe2ed3a317bcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/382 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3888165842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mds_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_tarea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m13635\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             dataset_dict[split] = dataset.map(\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3316\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3648\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3649\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3650\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3651\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3652\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3545\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3546\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3547\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3548\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3888165842.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mds_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_tarea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def predict(x):\n",
        "    inputs = preprocess_function(x)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        return {'prediction': predictions.item()}\n",
        "\n",
        "ds_predictions = ds_tarea.map(predict)\n",
        "\n",
        "assert len(ds_predictions['train']) == 13635\n",
        "assert len(ds_predictions['validation_matched']) == 413\n",
        "assert len(ds_predictions['validation_mismatched']) == 296\n",
        "\n",
        "ds_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7zea9jbmR2cu"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "    print(f\"*** {subset} ***\")\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "omaXJ0QTR2cu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "metrics = {}\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    pre = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    rec = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    metrics[subset] = [acc] + pre.tolist() + rec.tolist()\n",
        "    print(f\"Subset: {subset}:\")\n",
        "    print(f\"Accuracy: {acc:.2f} | Precision0: {pre[0]:.2f} | Precision1: {pre[1]:.2f} | Precision2: {pre[2]:.2f} | Recall0: {rec[0]:.2f} | Recall1: {rec[1]:.2f} | Recall2: {rec[2]:.2f}\")\n",
        "    print(\"-----\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bs4UC-OR2cu"
      },
      "source": [
        "### Criterio de evaluaci贸n\n",
        "\n",
        "La **nota final de la tarea1** estar谩 relacionada con el resultado de las m茅tricas de vuestro modelo en la combinaci贸n de *accuracy*, *precision* y *recall* para cada _split_ de datos.\n",
        "\n",
        "El criterio de evaluaci贸n ser谩 el siguiente:\n",
        "- La tarea1 se aprobar谩 si el notebook se entrega sin fallos y con un modelo entrenado (independientemente de sus m茅tricas).\n",
        "- La tarea1 tiene un 10 si se cumple que las m茅tricas de vuestro modelo entrenado igualan o superan los siguientes umbrales:\n",
        "\n",
        "| Subset               | Accuracy | Precision0 | Precision1 | Precision2 | Recall0 | Recall1 | Recall2 |\n",
        "|----------------------|----------|------------|------------|------------|---------|---------|---------|\n",
        "| validation_matched    | 0.78     | 0.78       | 0.76       | 0.85       | 0.80    | 0.77    | 0.81    |\n",
        "| validation_mismatched | 0.79     | 0.70       | 0.70       | 0.70       | 0.65    | 0.71    | 0.85    |\n",
        "\n",
        "- Por cada valor inferior a dicha m茅trica, la tarea pierde 0.5 puntos (m谩ximo 5.0 puntos de p茅rdida).\n",
        "\n",
        "Nota: La nota que se calcula a continuaci贸n es orientativa y podr铆a verse reducida en funci贸n del c贸digo de la entrega."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ne-yQSndR2cu"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def calculo_nota(metric):\n",
        "\n",
        "    vm_acc = float(metric['validation_matched'][0])\n",
        "    vm_pre0 = float(metric['validation_matched'][1])\n",
        "    vm_pre1 = float(metric['validation_matched'][2])\n",
        "    vm_pre2 = float(metric['validation_matched'][3])\n",
        "    vm_rec0 = float(metric['validation_matched'][4])\n",
        "    vm_rec1 = float(metric['validation_matched'][5])\n",
        "    vm_rec2 = float(metric['validation_matched'][6])\n",
        "    vmm_acc = float(metric['validation_mismatched'][0])\n",
        "    vmm_pre0 = float(metric['validation_mismatched'][1])\n",
        "    vmm_pre1 = float(metric['validation_mismatched'][2])\n",
        "    vmm_pre2 = float(metric['validation_mismatched'][3])\n",
        "    vmm_rec0 = float(metric['validation_mismatched'][4])\n",
        "    vmm_rec1 = float(metric['validation_mismatched'][5])\n",
        "    vmm_rec2 = float(metric['validation_mismatched'][6])\n",
        "\n",
        "    thresholds = {\n",
        "        'vm_acc': 0.78, 'vm_pre0': 0.78, 'vm_pre1': 0.76, 'vm_pre2': 0.85,\n",
        "        'vm_rec0': 0.80, 'vm_rec1': 0.77, 'vm_rec2': 0.81,\n",
        "        'vmm_acc': 0.79, 'vmm_pre0': 0.70, 'vmm_pre1': 0.70, 'vmm_pre2': 0.70,\n",
        "        'vmm_rec0': 0.65, 'vmm_rec1': 0.71, 'vmm_rec2': 0.85,\n",
        "    }\n",
        "    values = {\n",
        "        'vm_acc': vm_acc, 'vm_pre0': vm_pre0, 'vm_pre1': vm_pre1, 'vm_pre2': vm_pre2,\n",
        "        'vm_rec0': vm_rec0, 'vm_rec1': vm_rec1, 'vm_rec2': vm_rec2,\n",
        "        'vmm_acc': vmm_acc, 'vmm_pre0': vmm_pre0, 'vmm_pre1': vmm_pre1, 'vmm_pre2': vmm_pre2,\n",
        "        'vmm_rec0': vmm_rec0, 'vmm_rec1': vmm_rec1, 'vmm_rec2': vmm_rec2,\n",
        "    }\n",
        "\n",
        "    nota = 10\n",
        "    for key in thresholds:\n",
        "        if values[key] < thresholds[key]:\n",
        "            nota -= 0.5\n",
        "    return max(nota, 5.0)\n",
        "\n",
        "print(f\"Tu nota de la tarea1 es: {calculo_nota(metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OVWdiP2YR2cu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}